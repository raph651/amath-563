{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_1.png\" width =\"550\" height=\"300\" alt=\"centered image\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT3** : beta.openai.com/playground. *Few shot learning*, doesn't require training for the neuron networks\n",
    "\n",
    "**Alpha Fold 2**: Predict proteins \n",
    "\n",
    "**CLIP**: Based on image, predict the most relevant text discription\n",
    "\n",
    "**Dall-e 2**: Create image from text description. Create variations on image\n",
    "\n",
    "**PaLM**: For example, explaining a joke.\n",
    "\n",
    "## Staying up to date\n",
    "Newsletters: Andrew Ng's The Batch, AI2's newsletter\n",
    "\n",
    "Tech News: Engadget, Geekwire\n",
    "\n",
    "Conferences: NeurlPS-research, ODSC-industry\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_2.png\" width =\"550\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "**A few useful libraries**: MLflow, ONNX, Ray, Pytorch Lightning, Hugging Face\n",
    "\n",
    "**Cloud Solution**: Microsoft Azure ML, Amazon Sagemaker, Google Vertex AI\n",
    "\n",
    "**Github Upcoming Features**: GitHub Codespaces, GitHub Copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Sequence models, applications:\n",
    "* Speeach recognition\n",
    "* Music generation\n",
    "* DNA sequence analysis\n",
    "* Sentiment classificaiton\n",
    "* Machine translation\n",
    "* Name/Entity recognition\n",
    "* Activity recognition\n",
    "\n",
    "## Notations\n",
    "Direct Approach Cons: 1. Input & putput dimensions change for each input 2. Doesn't share features across sequence stepping 3. Large dimensions\n",
    "UniDirectional RNN: Introduce the initial activation that captures common units, then share features to the next step. \n",
    "\n",
    "## Propagation\n",
    "Three parameters: Wax, Waa, Wya \n",
    "\n",
    "$a^{<1>} = g_1(Waa* a^{<0>} + Wax *x^{<1>} +b_a),\\;\\; y^{<1>}=g_2(Wya *a^{<1>}+b_y) $\n",
    "\n",
    "$a^{<t>} = g_1(Waa* a^{<t-1>} + Wax *x^{<t>} +b_a),\\;\\; y^{<1>}=g_2(Wya *a^{<t>}+b_y) $\n",
    "\n",
    "Simplified Notation: \n",
    "\n",
    "$a^{<t>} = g(Wa[a^{<t-1>};x^{<t>}]+b_a) $\n",
    "\n",
    "$Wa =[Waa\\;|\\;Wax]$\n",
    "\n",
    "$[a^{<t-1>}; x^{<t>}]=[a^{<t-1>}][x^{<t>}] $\n",
    "\n",
    "Backward Propagation (through time)\n",
    "\n",
    "Types of RNNs: one to one, one to many, many to one, many to many   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Language Model\n",
    "\n",
    "Probability distribution over sequence of words. Given such a sequence say of length m, it assigns a probability to the whole sequence. \n",
    "\n",
    "P(W=SpaceX will **take** me to Mars) = p1\n",
    "\n",
    "P(W=SpaceX will **bake** me to Mars) = p2, $\\;\\;$ p2<<p1\n",
    "\n",
    "How to achieve this?\n",
    "\n",
    "Chain rule is used to estimate probability:\n",
    "\n",
    "$$P(w_1w_2...w_n)=\\prod_i P(w_i|w_1w_2...w_{i=1})$$\n",
    "\n",
    "P(W) = P(SpaceX) P(will|SpaceX) P(take|SpaceX will) P(me|SpaceX will take)...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "$y^{<t>}=g2(Wya * a^{<t>} +b_y) $\n",
    "\n",
    "$a^{<t>}=g1(Wa [a^{<t-1>}; x^{<t>}]+b_a) $\n",
    "\n",
    "g1 ~ tanh, g2 ~ tanh,...\n",
    "\n",
    "g1 is typically tanh because tanh is more stable over many time steps, relu is not differentiable so there's issue. Tanh is only practically more useful, but others might be good in different cases, for example classifications.\n",
    "\n",
    "### Block Diagram\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_3.png\" width =\"350\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### Vanishing/Exploding Gradients\n",
    "\n",
    "example: \"I grew up in France and moved to the United States, therefore I speak _____\"\n",
    "\n",
    "Make sure that we have to memorize the past words, keep tracking which is more important in the context.\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_4.png\" width =\"350\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "$W_a >1 $ blow up, $W_a<1$ vanish\n",
    "\n",
    "$sech(x) $ has maximun value 1, vanishing\n",
    "###  Ways to deal with Vanishing Gradientes \n",
    "\n",
    "* Gated Recurrent Unit (GRUs)\n",
    "\n",
    "$\\tilde{c^{<t>}}=g1(Wc [c^{<t-1>}; x^{<t>}]+b_c) $\n",
    "\n",
    "$\\Gamma_u =\\sigma(Wu[c^{<t-1>};x^{<t>}]+b_u )$ Update $\\Gamma$ gate\n",
    "\n",
    "$c^{<t>} =\\Gamma_u \\tilde{c^{<t>}} + (1-\\Gamma_u)c^{<t-1>} $\n",
    "\n",
    "$\\Gamma_u: $ \n",
    "0 --> $c^{<t>}=c^{<t-1>} $\n",
    "\n",
    "1 --> $c^{<t>}=\\tilde{c^{<t>}} $\n",
    "\n",
    "\n",
    "**Full GRU**\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_5.png\" width =\"350\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "* Long Short Term Memory (LSTM)\n",
    "\n",
    "Introduced $\\Gamma_f , \\Gamma_u, \\Gamma_o$\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_6.png\" width =\"350\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures of RNN Blocks\n",
    "\n",
    "* Bi-Directional RNN\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_7.png\" width =\"400\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "$$ \\hat{y}^{<1>} =g(Wy[a^{F<t>}; a^{B<t>}] +by) $$\n",
    "\n",
    "Pros: \n",
    "-Can learn long-range dependencies in the data\n",
    "-Can be trained on relatively small datasets\n",
    "\n",
    "Cons: \n",
    "-May struggle with learning short-range dependencies\n",
    "-Can be more difficult to train than traditional RNNs\n",
    "\n",
    "* Deep RNN\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_8.png\" width =\"400\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "There are several pros and cons of deep RNNs.\n",
    "\n",
    "On the positive side, deep RNNs can model complex non-linear relationships between input and output sequences. They can also learn long-term dependencies, which is difficult for shallow RNNs. In addition, deep RNNs are less likely to suffer from the vanishing gradient problem.\n",
    "\n",
    "On the negative side, deep RNNs can be difficult to train and optimize. They also tend to be more expensive to run, since they require more computational resources.\n",
    "\n",
    "\n",
    "* Encoder / Decoder RNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Sequence to Sequence\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_9.png\" width =\"400\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "Sequence might be huge if we consider tons of words. Grouping similar words could reduce the computations. \n",
    "\n",
    "\n",
    "* Attention\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_10.png\" width =\"400\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/6_11.png\" width =\"400\" height=\"250\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "The $\\alpha' s$  might be Softmax normalized. Integration of Fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Vanishing and Exploding Gradients\n",
    "\n",
    "* To learn on sequential data, we need \n",
    "    * Sensitivity to new input\n",
    "    * Retention of old information\n",
    "* Backpropagation over large number of steps leads to issues\n",
    "    * Amplification of patterns by repeated applicaiton of same function\n",
    "* Long-term memory of RNNs suffers due to uncontrolled gradients\n",
    "\n",
    "* Additional mechanisms are used to optimize memory and sensitivity \n",
    "    * Gated Structure : GRU, LSTM\n",
    "    * Constrained-weight RNNs\n",
    "\n",
    "* RNN performance can be very sensitive to initialization: Hidden size, Non-linearity,input statistics\n",
    "\n",
    "* torch.nn.init \n",
    "\n",
    "* Teacher FOrcing\n",
    "    when predicting a sequence from an input sequence, you can use \"teacher forcing\". It randomly selects either actual input value or an ouput from previous step\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42bb6a2c1686de5e1e448fe975e26b056eae8a02500337bf312c2103fe0e4268"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
