{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ad0fab",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "$J=\\displaystyle \\sum_{i=1}^n(\\vec{p}\\cdot{\\vec{\\tilde{x_i}}}-y_i)^2$\n",
    "\n",
    "$\\displaystyle \\vec{p}^* = arg \\min_{\\vec{p}} J(\\vec{p})$\n",
    "\n",
    "$ \\displaystyle \\forall i :\\frac{\\partial J}{\\partial p_j}=0 \\;\\;\\quad \\frac{\\partial J}{\\partial p_j} =\\sum_{i=1}^n 2(\\vec{p}\\cdot{\\vec{\\tilde{x_i}}}-y_i)\\vec{\\tilde{x_i}}$\n",
    "\n",
    "solution is $\\displaystyle \\quad \\vec{p}^*= (X^TX)^{-1}X^T\\vec{y}$\n",
    "\n",
    "\n",
    "\n",
    "**Over-determined**:\n",
    "\n",
    "$\\displaystyle \\quad arg \\min_{\\vec{p}}(||A\\vec{p}-\\vec{b}||_2 +\\lambda g(p)) $\n",
    "\n",
    "**Under-determined**:\n",
    "\n",
    "$\\displaystyle \\quad arg \\min_{\\vec{p}} g(p) \\quad$ subject to  $||A\\vec{p}-\\vec{b}||_2\\le \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6498856",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "$\\displaystyle L(\\hat{y},y)=\\frac{1}{N} \\sum_i (\\hat{y}_i -y_i)^2 \\quad$   MSE\n",
    "\n",
    "$\\displaystyle L(\\hat{y},y)=-(y \\,\\text{log}\\hat{y} +(1-y)\\,\\text{log}(1-\\hat{y})) \\quad $ Cross Entropy\n",
    "\n",
    "$\\displaystyle L(\\hat{y},y)=-\\sum_c(y_{o,c})\\,\\text{log}p_{o,c} \\quad $ Cross Entropy Multi class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07acad9",
   "metadata": {},
   "source": [
    "## Cost \n",
    "\n",
    "$\\displaystyle J(W,b) = \\frac{1}{M}\\sum_{i=1}^m L(\\hat{y}^{(i)},y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48baef1",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We'll start with single nueron logistic regression using activation function **sigmoid**\n",
    "\n",
    "* Loss Cross Entropy\n",
    "* Maximum Likelihood\n",
    "* Convex Optimization\n",
    "\n",
    "### Computation Graph\n",
    "\n",
    "$\\hat{y} = \\sigma(\\vec{w}^T\\vec{x} + b) \\rightarrow $ cross entropy $\\rightarrow L(\\hat{y},y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466692a",
   "metadata": {},
   "source": [
    "## Multiple Examples Training set\n",
    "\n",
    "* **Forward Propagation**: Computing the loss through forward pass for a single training example\n",
    "* **Backward Propagation**: Computing gradients of parameters through backward pass for a single training example $\\\\$\n",
    "\n",
    "* **Batch**: Traning set could be divided into smaller sets called batches\n",
    "* **Iteration**: When an entire batch is passed both forward and backward\n",
    "* **Epoch**: When an entire dataset is passed both forward and backward through the NN once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb236d",
   "metadata": {},
   "source": [
    "## Multiple Outputs \n",
    "Sigmoid -> softmax with one hot encoding\n",
    "\n",
    "$softmax (\\hat{y})_i= \\frac{e^{y_i}}{\\sum_ie^{y_i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc3dac",
   "metadata": {},
   "source": [
    "## Other Loss/Activations\n",
    "* Changing **activation** will change the gradients\n",
    "\n",
    "* Changing **loss** will change the gradients and could make the composition unseparable\n",
    "\n",
    "* Most activations we discussed have analytic gradients. It is possible that there is no analytic expression. \n",
    "\n",
    "* Gradients could be evaluated numerically\n",
    "    - When **analytic** expression is not available\n",
    "    - When it is **faster** to evaluate them numerically\n",
    "    \n",
    "* Use central difference formula \n",
    "* Check against analytic gradient for several examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cbf11",
   "metadata": {},
   "source": [
    "<img src=\"images/2_1.png\" width =\"500\" height=\"350\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d7d7b",
   "metadata": {},
   "source": [
    "## Curriculum Learning \n",
    "* Training machine learning models with particular order. Starting with easier subtasks and gradually increase the difficulty level of the tasks (For example, NLP problem learn words and then learn sentences)\n",
    "\n",
    "* Both traning set and cost functions aree updated accordingly\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "**Almost surely convergence**\n",
    "\n",
    "* Performs an update for each training example $x^{(i)}$ and label $y^{(i)}$\n",
    "\n",
    "* The values of the loss and parameters will fluctuate\n",
    "    - (+) will discover better minimums\n",
    "    - (-) convergence to chosen minimum will keep overshooting\n",
    "\n",
    "* Learning rate plays a very important role\n",
    "\n",
    "## Mini-batch GD\n",
    "\n",
    "$$w_{k+1} =w_k -\\alpha \\cdot \\nabla_wJ(w;x^{(i:i+n)};y^{(i:i+n)})$$\n",
    "\n",
    "* Mini-batch GD is a hybrid method between GD and SGD. \n",
    "* Performs an update for every mini-batch of n traning examples.\n",
    "\n",
    "    - (+) reduces the variance of the parameter updates\n",
    "    - (+) efficient in computing the gradient w.r.t a mini-batch\n",
    "* mini-batch sizes range between 50-256\n",
    "\n",
    "**Challenges**\n",
    "* Chossing a proper learning rate can be difficult\n",
    "\n",
    "* Learning rate smart schedule\n",
    "    - Annealing\n",
    "    - Change of J below threshold\n",
    "* Variable learning for different parameters\n",
    "* Suboptimal local (saddle points)\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "* Model Parameters: W, b, activation, output, cost\n",
    "\n",
    "* Hyper-parameters: Batch/minibatch size, learning parameters, external parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2559693",
   "metadata": {},
   "source": [
    "## Methods for choosing learning rate\n",
    "**1. Learning rate decay**\n",
    "\n",
    "* $\\displaystyle \\alpha = \\frac{\\alpha_0}{1+\\text{decr}\\cdot\\text{epnum}}$\n",
    "\n",
    "* $\\displaystyle \\alpha = d^{\\text{epnum}}\\cdot \\alpha_0$\n",
    "\n",
    "* $\\displaystyle \\alpha = \\frac{d\\cdot\\alpha_0}{\\sqrt{\\text{epnum}}}$\n",
    "\n",
    "**2. Momentum Method**\n",
    "\n",
    "$$ v_{k+1} =\\gamma v_k+\\alpha\\cdot\\nabla_wJ(w_k)$$\n",
    "$$w_{k+1} =w_k -v_{k+1}$$\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/2_2.png\" width =\"500\" height=\"150\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "**3. Nesterov Accelerated Gradient**\n",
    "\n",
    "$$v_{k+1}=\\gamma v_k +\\alpha \\cdot\\nabla_wJ(w_k-\\gamma v_k)$$\n",
    "$$w_{k+1}=w_k-v_{k+1}$$\n",
    "\n",
    "\n",
    "**4. Adagrad**\n",
    "$$w_{k+1,j} =w_{k,j} -\\frac{\\alpha}{\\sqrt{G_{k,jj}+\\epsilon}}\\cdot g_{k,j}$$\n",
    "\n",
    "g is our gradient\n",
    "\n",
    "* Adagrad uses a different learning rate for every parameter $w_j$ at every step $k$. $G$ is diagonal matrix of sum squared gradient values.\n",
    "\n",
    "* Performs smaller update (i.e. low learning rates) for parameters associated with frequntly occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.\n",
    "\n",
    "**5. RMSProp**\n",
    "\n",
    "$$E[g^2]_k =\\gamma E[g^2]_{k-1} + (1-\\gamma)g_k^2$$\n",
    "$$w_{k+1} =w_k -\\frac{\\eta}{\\sqrt{E[g^2]_k+ \\epsilon}}g_k$$\n",
    "\n",
    "* Prevents accumulation by adding regularizing term in the running average (exponentially decaying)\n",
    "\n",
    "* Beneficial for RNNs\n",
    "\n",
    "**6. Adadelta**\n",
    "\n",
    "\\begin{align}\n",
    "E[\\Delta w^2]_k = \\gamma E[\\Delta &w^2]_{k-1} +(1-\\gamma) \\Delta w_k^2\\\\\n",
    "\\text{RMS}[\\Delta w]_k &=\\sqrt{E[\\Delta w^2]_k +\\epsilon}\\\\\n",
    "                    \\Delta w_k    &=-\\frac{\\text{RMS}[\\Delta w]_{k-1}}{\\text{RMS}[g]_k}g_k\\\\\n",
    "                    w_{k+1}&=w_k+\\Delta w_k\n",
    "\\end{align}\n",
    "\n",
    "* Generalizes RMSProp /Adagard for considering RMS instead of accumulation of grad\n",
    "\n",
    "* No learning rate parameter\n",
    "\n",
    "**7. AdaM - Adaptive Moment Estimation**\n",
    "\n",
    "\\begin{align}\n",
    "m_k &=\\beta_1m_{k-1} + (1-\\beta_1)g_k\\\\\n",
    "v_k &=\\beta_2v_{k-1} +(1-\\beta_2)g_k^2\\\\\n",
    "w_{k+1}&=w_k -\\frac{\\eta}{\\sqrt{\\hat{v}_k}+\\epsilon}\\hat{m}_k\n",
    "\\end{align}\n",
    "\n",
    "* Keeps track of 2 moments: mean and variance\n",
    "* Normalizes them to prevent biases\n",
    "    - $\\displaystyle \\hat{m}_k =\\frac{m_k}{1-\\beta_1^k}$\n",
    "    - $\\displaystyle \\hat{v}_k =\\frac{v_k}{1-\\beta_2^k}$\n",
    "\n",
    "**Additional**\n",
    "- AdaMax: Generalization of AdaM to L-infinity norm\n",
    "- Nadam: Nesterov AdaM\n",
    "- AMSgrad: Max normalization instead of exponential in AdaM\n",
    "\n",
    "**Notes on Choosing Opimizers**\n",
    "\n",
    "* RSMProp & AdaDelta adaptive\n",
    "* AdaM adaptive + momentum  -> robust\n",
    "* SGD as a first pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2d85c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42bb6a2c1686de5e1e448fe975e26b056eae8a02500337bf312c2103fe0e4268"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
