{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current week: The Data Driven Deep Leanring Process\n",
    "\n",
    "### Exponentially Moving Average (EMA)\n",
    "\n",
    "$$ v_k = \\gamma v_{k-1} + (1-\\gamma)\\theta_k, \\; v_0=0$$\n",
    "\n",
    "Iterate through steps:\n",
    "\n",
    "\\begin{align*}\n",
    "v_1&=(1-\\gamma)\\theta_1\\\\\n",
    "v_2&=\\gamma(1-\\gamma)\\theta_1 +(1-\\gamma)\\theta_2\\\\\n",
    "v_3&=\\gamma^2(1-\\gamma)\\theta_1+\\gamma(1-\\gamma)\\theta_2+(1-\\gamma)\\theta_3\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Applying it to **gradient** values:\n",
    "\n",
    "$$ \\hat{v}_k =\\gamma \\hat{v}_{k-1} +(1-\\gamma)\\nabla_{w_k}$$\n",
    "$$w_{k+1}=w_k-\\alpha\\hat{v}_k$$\n",
    "\n",
    "\n",
    "#### RMS Prop\n",
    "\n",
    "Let's apply EMA to **square gradient** values:\n",
    "\n",
    "$$ S_k=\\gamma S_{k-1}+(1-\\gamma)(\\nabla_{w_k})^2$$\n",
    "$$w_{k+1}=w_k-\\frac{\\alpha}{\\sqrt{S_k}+\\epsilon}\\nabla_{w_k}$$\n",
    "\n",
    "#### Biases in EMA Correction\n",
    "\n",
    "Correcton: $\\displaystyle \\tilde{v}_k = \\frac{v_k}{1-\\gamma^{k}}$\n",
    "\n",
    "#### Adam\n",
    "\n",
    "What if we combine Momentum with RMSProp\n",
    "\n",
    "$$\\hat{v}_k = \\gamma_1 \\hat{v}_{k-1} +(1-\\gamma_1)\\nabla_{wk}$$\n",
    "$$S_k =\\gamma_2 S_{k-1} +(1-\\gamma_2)(\\nabla_{wk})^2$$\n",
    "\n",
    "And do bias correction\n",
    "\n",
    "$$\\hat{v}_k^c = \\frac{\\hat{v}_k}{1-\\gamma_1^k}$$\n",
    "\n",
    "$$ w_{k+1}=w_k-\\frac{\\alpha}{\\sqrt{S_k^c}+\\epsilon}\\hat{v}_k^c$$\n",
    "\n",
    "$$S_k^c = \\frac{S_k}{1-\\gamma_2^k} $$\n",
    "\n",
    "hyperparameters: $\\alpha$, $\\gamma_1$, $\\gamma_2$, $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation \n",
    "\n",
    "Train/Validation/Test set division:  \n",
    "\n",
    "if total 10k, 8k/1k/1k\n",
    "\n",
    "if total 1M, 960k/20k/20k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### L2 norm regularization\n",
    "\n",
    "### Dropout Regularization\n",
    "Analogous to L2 norm ergularization, Dropout regularization restricts the optimization to fewer parameters. It basically apply a random probability map to the nodes, making some unimportant (small) and keeping the others important (intact).\n",
    "\n",
    "\n",
    "### Additional Regularization\n",
    "\n",
    "**Data Augmentation**:\n",
    "\n",
    "Enhance the data set with additional manipulations. \n",
    "\n",
    "* General input: add noise/distortions, synthetic\n",
    "\n",
    "* Images: resolutions, rotate, add symmetries \n",
    "\n",
    "* Shapes/digits: distort\n",
    "\n",
    "So the idea is about getting more data points based on exsiting data.\n",
    "\n",
    "**Early Stopping**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients\n",
    "\n",
    "* **Very deep** neural network\n",
    "without activation\n",
    "\n",
    "$\\displaystyle \\hat{y}=w_L\\cdot ... w_l \\cdot ...w_2 \\cdot w_1\\cdot x$\n",
    "\n",
    "if $w>1, \\;\\; \\hat{y} \\to \\infty\\\\$\n",
    "if $w<1, \\;\\; \\hat{y} \\to 0$\n",
    "\n",
    "with activation: forward propagation and backward propagation are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of the datasets\n",
    "\n",
    "* Zero mean\n",
    "\n",
    "* Normalized Variance\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Normalize the outputs of each layer.\n",
    "\n",
    "* To make sure that the layers outputs will not be forced to be zero mean and variance 1\n",
    "\n",
    "$$\\tilde{z}^{[l](i)} = \\gamma^{[l]}z_{norm}^{[l](i)}+\\beta^{[l]}$$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learned in the process\n",
    "\n",
    "\n",
    "* Forward propagation\n",
    "\n",
    "$$ a^{[l-1](i)} \\; \\rightarrow\\; z^{[l](i)}\\;\\rightarrow\\;_{BN}\\tilde{z}^{[l](i)}\\;\\rightarrow\\; a^{[l](i)}$$\n",
    "\n",
    "* Can do per mini-batch\n",
    "\n",
    "\n",
    "**Pseudo-code**\n",
    "\n",
    "For each mini-batch, compute F-prop, in each layer replace $z^{[l](i)}$ by $\\tilde{z}^{[l](i)}$. Do B-prop to compute $\\nabla_{w^{[l]}},\\nabla_{\\beta^{[l]}},\\nabla_{\\gamma^{[l]}}$. Update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "* Zero -> Problematic\n",
    "\n",
    "* Random Normal (0,1) -> Problematic (vanishing gradients, for example)\n",
    "\n",
    "* Xavier (tanh):\n",
    "    $$Var(w^{[l]}): 1/n^{[l-1]}     $$\n",
    "    $$w^{[l]} = N(0,1)\\cdot \\sqrt{\\frac{1}{n^{[l-1]}}}                            $$\n",
    "    \n",
    "* He (ReLU):\n",
    "    $$Var(w^{[l]}): 2/n^{[l-1]}$$\n",
    "    $$w^{[l]} = N(0,1)\\cdot \\sqrt{\\frac{2}{n^{[l-1]}}} $$ \n",
    "    , the factor 2 is found to be more useful in practice\n",
    "\n",
    "* Other: \n",
    "\n",
    "    $$Var(w^{[l]}): \\frac{2}{n^{[l-1]}+n^{[l]}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "* Grid Search, Random Search\n",
    "\n",
    "* Coarse to fine\n",
    "\n",
    "* Linear for: $L, n^{[l]}$\n",
    "\n",
    "* $log_{10}$ scale for"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
