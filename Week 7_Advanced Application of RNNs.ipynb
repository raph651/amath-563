{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Applications\n",
    "\n",
    "\n",
    "\n",
    "### Word Embeddings:\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/7_1.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "One hot encoding doesn't represent relaions between words, and it's consuming a lot space. Use word embeddings to connect words and build their relations.\n",
    "\n",
    "**Visualization -> t-SNE (t-distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "Reducing high-dimensional embeddings into a lower dimensional space. \n",
    "\n",
    "**Can define distances (similarity)**:\n",
    "\n",
    "$e_{Man}-e_{Woman}\\approx [-2;0;0...;0] $\n",
    "\n",
    "$e_{King}-e_{Queen}\\approx [-2;0;0...;0] $ \n",
    "\n",
    "So Man-Woman $\\approx$ King-Queen.\n",
    "\n",
    "**Similarity**:\n",
    "\n",
    "$sim(u,v) =\\frac{u^Tv}{||u|||\\,||v||} $\n",
    "\n",
    "$sim(u,v)=||u-v||^2 $\n",
    "\n",
    "**Context and Target**\n",
    "\n",
    "\"I want a glass of orange juice to go along with my cereal\"\n",
    "\n",
    "target : juice \n",
    "\n",
    "context: a glass of orange, orange, glass...\n",
    "\n",
    "### Word2Vec (skip gram):\n",
    "\n",
    "context -> E -> $e_c$ -> SM -> $y_t$\n",
    "\n",
    "analyze each window of context, scanning the whole vocabulary\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "$P(w_i) =\\frac{f(w_i)^{3/4}}{\\sum^{N_v}f(w_i)^{3/4}} $\n",
    "\n",
    "random choose words then picks the best related\n",
    "\n",
    "### RNN Sentiment Classification\n",
    "\n",
    "Use pre-trained Embedding matrix to embed the input. The translated input goes into RNN as sequential data, at the end of the sentence, Softmax output the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "* Time stamp format - time alignment: Put features on the same time line, and train these features altogether\n",
    "* Proxy timeseries. For example, use daily samples and weekly mean samples at the same time\n",
    "### Filtering task:\n",
    "\n",
    "Many to many RNN problem. Use noisy signal as input, output the denoised/original signal.This type of denosing task requires a great amount of sample datas, so the task is still challenging. If actual signal is not continous, RNN could get much lower performance since it's hard for RNN structure to transform one block to another with discontinuity. Fixed noise distribution. Loss will determine filtering results. Need to re-train for different filtering tasks.\n",
    "### Completion\n",
    "Sample is corrupted at specific time. \n",
    "1. Sample is corrupted at fixed length/periodically. Bi-directional Many to one/Many to many problem. \n",
    "2. Sample is corrupted at a few random time. Hybrid of one to many & many to many problem. Start from an input sequence x1, predict the next step x2, and then take x2 as input to predict the followings. \n",
    "One to many + attention. Ablation. Regularization. Multivariate setup.\n",
    "* Multi variate setup\n",
    "\n",
    "* Extension of signal dimensions\n",
    "    - Derivatives\n",
    "    - Mean and higher moments\n",
    "    - Superpositions of sub-signals\n",
    "    - Transforms (global transform)\n",
    "    - Windowed transforms (EMA, local DWT)\n",
    "* Projections of Signal dimensions\n",
    "    - Projections (PCA, FFT truncation)\n",
    "    - Embedding\n",
    "    - Proxy timeseries  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "\n",
    "From past predict future.  For example stock price prediction. \n",
    "\n",
    "**Moving Prediction - Forcasting**:\n",
    "\n",
    "Use a sliding window, for example first 100 inputs, to predict the next. Then move the window by one time step, instead of remembering all the past, only predicting on a specific interval. \n",
    "* One step prediciton/ many to one.  \n",
    "\n",
    "* Long-Term Prediction (Translation)\n",
    "Given sequence to output sequence\n",
    "\n",
    "* Sequence to Sequence \n",
    " **Reconstruction**\n",
    " input -> (auto) encoder -> embedding -> FC layers -> (auto) decoder -> output\n",
    "\n",
    " Input == output. \n",
    "\n",
    " The idea is that reconstruction optimizes for an embedding or feature layers that include information of inputs. So if input is disconnected, the information is still intact.\n",
    "\n",
    " * Signal Validity (Anomaly Detection)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
