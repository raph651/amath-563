{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Singular Value Decomposition (SVD)\n",
    "* Principal Component Analysis (PCA)\n",
    "* Proper Orthogonal Decomposition (POD)\n",
    "* Dynamic Mode Decomposition (DMD)\n",
    "* Time-delayed Embeddings\n",
    "\n",
    "## Linear Transformations\n",
    "\n",
    "$x = \\left[\\begin{array}{ccc} \n",
    "1 \\\\\n",
    "3\n",
    "\\end{array}\\right],\\; \\mathbf{A}=\\left[\\begin{array}{ccc} \n",
    "2 & 1 \\\\\n",
    "-1 & 1\n",
    "\\end{array}\\right] \\rightarrow \\,y=\\mathbf{Ax}=\\left[\\begin{array}{ccc} \n",
    "5 \\\\\n",
    "2\n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Rotation**\n",
    "$\\mathbf{A}=\\left[\\begin{array}{ccc} \n",
    "\\text{cos}\\theta & \\text{-sin}\\theta \\\\\n",
    "\\text{sin}\\theta & \\text{cos}\\theta\n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Scaling**\n",
    "$\\mathbf{A}=\\left[\\begin{array}{ccc} \n",
    "\\alpha & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{array}\\right]$\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/8_1.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "**SVD Properties**\n",
    "\n",
    "$\\mathbf{A}=\\mathbf{U\\Sigma V^*} $\n",
    "\n",
    "$\\mathbf{U} \\in \\mathbb{C}^{mxm}$ is unitary<br>\n",
    "$\\mathbf{V} \\in \\mathbb{C}^{nxn}$ is unitary<br>\n",
    "$\\mathbf{\\Sigma} \\in \\mathbb{R}^{mxn}$ is diagonal\n",
    "\n",
    "**Computing SVD - Eigenvalur Problem**\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathbf{A^TA} &= \\mathbb{U\\Sigma V^*}^T\\mathbf{U\\Sigma V^*}\\\\\n",
    "& = \\mathbf{V\\Sigma^2V^*}\n",
    "\\end{align}$\n",
    "\n",
    "$\\mathbf{AA^T}= \\mathbf{U\\Sigma^2U^*}\\;\\;$ This then will become: $(AA^T)U= U\\Sigma^2$.\n",
    "So eigenvalue is $\\Sigma^2$, eigenvector is $U$\n",
    "\n",
    "\n",
    "**Question**: Are unitary matrices U, V unique? Are the singular values unique? Explain and provide several examples if they are not.\n",
    "\n",
    ">   No, unitary matrices U, V are not unique. The singular values are unique. Given an A matrix, the function of A acting on any vector is to rotate that vector and scale it by some factor. <br><br>\n",
    "A = UΣV* acting on any vector means first inverse rotate it by V, scale it, then rotate by U. Suppose we rotate a vector by 20 degrees, and at the mean time scale it by factor of 3. We can first rotate it by 10 degrees, scale it by factor of 3, then rotate another 10 degrees. Or, we can first rotate it by 30 degrees, scale it by factor of 3, then rotate back 10 degrees. There are many more ways to rotate, but only one way to scale it because we only scale it once. So U, V are not unique. Singular values are unique.<br><br>\n",
    "Another proof of non-uniqueness of U, V: <br> $A = U\\Sigma V^* = U\\Sigma WW^*V^* = (UW)\\Sigma (W^*V^*) = (UW)\\Sigma (VW)^*$\n",
    "So U, V could be replaced by any other combinations with W, where W is a unitary matrix.\n",
    "\n",
    "**Properties of SVD**\n",
    "\n",
    "* **Theorem**: The *nonzero singular values* of $\\mathbf{A}$ are the square roots of the nonzero eigenvalues of $\\mathbf{A^*A}$ or $\\mathbf{AA^*}$ \n",
    "* **Theorem**: The norm $||\\mathbf{A}||_2 =\\sigma_1$ (max eigenvalues) and $||\\mathbf{A}||_F = \\sqrt{\\sigma_1^2+\\sigma_2^2+ \\cdot\\cdot\\cdot+\\sigma_r^2} $\n",
    "* **Theorem**: For any N so that $0\\le N\\le r$, we can define the partial sum \n",
    "\n",
    "    $$\\mathbf{A}_N =\\sum_{j=1}^N \\sigma_j \\mathbf{u}_j\\mathbf{v}_j^* $$\n",
    "    $||\\mathbf{A}-\\mathbf{A_N}||_N = \\sqrt{\\sigma_{N+1}^2+\\sigma_{N+2}^2+\\cdot\\cdot\\cdot+\\sigma_r^2} $\n",
    "\n",
    "\n",
    "## PCA \n",
    "subtract the mean for all variables, define the covariance matrix $X^TX$. \n",
    "\n",
    "**Embedding** \n",
    "Project each point on Principal axies - low dimension embedding\n",
    "\n",
    "## POD\n",
    "\n",
    "$ f(x,t)\\approx \\sum_{j=1}^N a_j(t)\\phi_j(x) \\rightarrow \\; F(x,t)\\approx U(x)\\Sigma V(t)^T $ \n",
    "\n",
    "## DMD\n",
    "\n",
    "DMD: Computes a set of **modes** associated with fixed **oscillation frequencies** and **decay/growth** rates for given multi dimensional time series.\n",
    "\n",
    "Predict the next snapshot from previous one:  $v_{i+1}=Av_i $\n",
    "\n",
    "## Time Delay Embeddings\n",
    "\n",
    "Build delay matrix for variable $x_1$, $H = [x_1;x_1^\\delta;...;x_1^{(k+1)\\delta}] \\;\\; H =U\\Sigma V^T$\n",
    "Takens's Therorem: Attractor with dim *d* can be embedded in Euclidean space with *k>2d* lags.\n",
    "\n",
    "Example: Lorentz attractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Embedding - Truncation\n",
    "\n",
    "d dimension -> lower (2d) dimension\n",
    "\n",
    "## Manifold Learning\n",
    "\n",
    "**manifold learning**: a subfiled of machine learning closely related to dimension reduction based on the assumption that one's observed data lie on a low-dimensional manifold embedded in a higher-dimensional space.\n",
    "\n",
    "* Manifold Visualization\n",
    "* Manifold approximation\n",
    "\n",
    "## Multidimensional Scaling (MDS)\n",
    "\n",
    "Multidimensional scaling is a **visual representation of distances or dissimilarities between sets of objects** \"Objects\" can be any kind of real or conceptual stimuli (e.g. colors, faces, map coordinates, etc).\n",
    "\n",
    "* Objects that are more similar (or have shorter distances) are closer together on the graph than objects that are less similar (or have longer distances).\n",
    "* MDS can also serve as a dimension reduction technique for high-dimensional data, as well as interpreting dissimilarities as distances on a graph.\n",
    "\n",
    "**MDS Algorithm**\n",
    "\n",
    "Compute pairwise distance between all pairs of points\n",
    "\n",
    "$D:= \\left[\\begin{array}{ccc} \n",
    "d_{1,1} & d_{1,2} & ... & d_{1,M}\\\\\n",
    "d_{2,1} & d_{2,2} & ... & d_{2,M} \\\\\n",
    ". &. & &.\\\\\n",
    ". &. & &.\\\\\n",
    ". &. & &.\\\\\n",
    "d_{M,1} & d_{M,2} &... & d_{M,M}\n",
    " \\end{array}\\right]$\n",
    "\n",
    " $d_{ij} = ||x_i-x_j|| $\n",
    "\n",
    " Center the matrix D (double centering in rows and columns)  $\\tilde{D} -> EVD(U\\sqrt{\\Sigma})$\n",
    "\n",
    "\n",
    "\n",
    "**Question**: Consider $D_{n\\times n}=[d_{ij}^2], $ a matrix of squared pairwise Euclidean distances between variables in the matirx $X_{n\\times k}$ (rows are variables, columns are observations). Show that eigenvalues $\\lambda_m$ and eigenvectors $\\vec{u_m}$ of $\\tilde{D}_{n\\times n}$, a matrix $D$ that is centered in rows and columns, correspond to $\\lambda_m=s_m^2,\\vec{u_m}$ respectively, where $s_m$ are diagonal elements in $S$ and $\\vec{u_m}$ are vectors in $U$, such that $X=USV^T,$ i.e. SVD of a centered matrix $X.$\n",
    "\n",
    "This thus shows that MDS (finding representation that preserves Euclidean distances of points in $X$) and PCA of $X$ are equivalent.\n",
    " \n",
    "\n",
    "> Consider the $D$ matrix, with $d_{ij}= ||x_i-x_j||^2$, where $\\displaystyle X    =\\left(\\begin{array}{ccc}  \n",
    "   x_1\\\\\n",
    "   \\hline\n",
    "   x_2\\\\\n",
    "   \\hline\n",
    "   .\\\\.\\\\.\\\\\\hline x_n\n",
    "   \\end{array}\\right) $<br>\n",
    "   $d_{ij}= ||x_i-x_j||^2 = x_i^2 -2x_i\\cdot x_j+x_j^2$.<br><br>\n",
    "   Specifically, $D := \\left(\\begin{array}{ccc}  \n",
    "   0 & ||x_1-x_2||^2 & ...\\\\\n",
    "   ||x_2-x_1||^2 & 0 & ...\\\\\n",
    "   ...\\\\...\\\\\n",
    "   ||x_n-x_1||^2& ...&0\n",
    "   \\end{array}\\right)$<br><br>\n",
    "   We want to double center the $D$ matrix, in a way that:<br><br>\n",
    "   Sum of rows $s_i = \\displaystyle \\sum_j d_{ij}^2 = \\displaystyle \\sum_j  x_i^2 -2x_i\\cdot x_j+x_j^2 = nx_i^2-2x_i\\cdot\\sum_j x_j + \\sum_jx_j^2$.<br><br>\n",
    "   Sum of all entries in $D:$ $$s= \\displaystyle \\sum_i s_i = n\\sum_i x_i^2 -2\\sum_i x_i \\cdot \\sum_j x_j+ \\sum_i\\sum_jx_j^2=2n\\sum_ix_i^2-2\\sum_i x_i \\cdot \\sum_j x_j$$<br>\n",
    "   Centering by rows such that $\\displaystyle\\sum_i x_i =0$, we get:<br><br>\n",
    "   $s_i=nx_i^2+\\displaystyle\\sum_jx_j^2$ <br>\n",
    "   $s=\\displaystyle 2n\\sum_i x_i^2$<br><br>\n",
    "   Now we have, $$d_{ij}-\\frac{s_i+s_j-\\frac{1}{n}s}{n} = -2x_i\\cdot x_j$$<br>\n",
    "   Divide this by -2, we have $\\tilde{D}$ matrix, making the equation above to be, $RHS= x_i\\cdot x_j$<br><br>\n",
    "   Consider the matrix $XX^T$, each entry $(XX^T)_{ij} = x_i\\cdot x_j$.<br><br>\n",
    "   Thus the two matrices are equivalent. The results of SVD must be equivalent too. And the singular value for $A$ is indeed the square root of the singular value for $\\tilde{D}$ with the same eigenvectors.\n",
    "\n",
    "\n",
    "\n",
    " ## ISOMAP\n",
    "\n",
    " **Isomap: nonlinear dimensionality reduction** estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point's neighbors on the manifold.\n",
    "\n",
    " * **Determine the neighbors of each point**<br>\n",
    "    All points in some fixed radius<br>\n",
    "    K nearest neighbors\n",
    "* **Construct a neigborhood graph**<br>\n",
    "   Each point is connected to other if it is a *K* nearest neighbor<br>\n",
    "   Edge length equal to Euclidean distance\n",
    "* **Compute shortest path between two nodes**<br>\n",
    "   Dijkstra's algorithm<br>\n",
    "   Floyd-Warshall algorithm\n",
    "* **Compute lower-dimensional embedding**<br>\n",
    "   Multidimensional scaling\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE: T Stochastic Neighborhood Embedding\n",
    "\n",
    "Optimization approach. \n",
    "\n",
    "**Crowding Problem**: It is impossibl to preserve distance in all Neighborhoods<br>\n",
    "**Possible solution**: t-distribution\n",
    "\n",
    "If points are crowded, t-distribution looks like standard normal distribution. If they are sparse, t-distribution is more flat and smooth.\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/8_2.png\" width =\"450\" height=\"350\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "## (Temporal) Force Directed Graphs\n",
    "\n",
    "Embedding -> Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Clustering\n",
    "\n",
    "* UMAP\n",
    "* Clustering\n",
    "* K Means\n",
    "* K Nearest Neighbors (KNN)\n",
    "\n",
    "## UMAP\n",
    "**Uniform Manifold Aapproximation and Projection**<br>\n",
    "\n",
    "Mclnnes, Leland, John Healy, and James Melville. Umap(2018)<br>\n",
    "\n",
    "**Neighbor Graph**\n",
    "\n",
    "one point - 0 simplex, two points connected - 1 simplex\n",
    "\n",
    "The idea is that define a ball of radius r for each data point. For sparse region, the data points have larger radius. For dense region, they have smaller radius. It is complicated to measure the density of data points, we instead find the associated k nearest neighbors. This is a hyperparameter to tune. \n",
    "\n",
    "Example: UMAP on MNIST hand-written digits, unsupervised learning. \n",
    "\n",
    "## Clustering\n",
    "\n",
    "### K-Means\n",
    "* Unsupervised \n",
    "* K - number of clusters (centers)\n",
    "* Edclidean Distance\n",
    "* Iterative Process\n",
    "\n",
    "* An iterative clustering algorithm\n",
    "- Initialize: Pick *K* random points as cluster centers\n",
    "- Alternate:\n",
    "    1. Assign data points to closest cluster center\n",
    "    2. Change the cluster center to the average of its assigned points\n",
    "- Stop when no points' assignments change\n",
    "\n",
    "**Properties of K-means algorithm**\n",
    "* Guaranteed to converge in a finite number of iterations\n",
    "* Running time per iteration\n",
    "    1. Assign data points to closest cluster center<br>\n",
    "    O(KN) time\n",
    "    2. Change the cluster...<br>\n",
    "    O(N)\n",
    "* Symmetric\n",
    "    - D(A,B) =D(B,A)\n",
    "    - Otherwise, we can say A looks like B but B does not look like A\n",
    "* Positivity, and self-similarity\n",
    "    - D(A,B) $\\ge$ 0, and D(A,B)=0 iff A=B\n",
    "    - Otherwise there will be different objects that we cannot tell apart\n",
    "* Triangle inequality\n",
    "    - D(A,B) +D(B,C) $\\ge$ D(A,C)\n",
    "    - Otherwise one can say \"A is like B, B is like C, but A is not like C at all\"\n",
    "\n",
    "**K-means Convergence**: K-means taks an alternating optimization approach, each step is guaranteed to decrease the objective - thus guaranteed to converge\n",
    "\n",
    "**K-Means Initialization**\n",
    "* K-means algorithm is a heuristic\n",
    "    - Requires inital means\n",
    "    - It does matter what you pick!<br>\n",
    "    - What can go wrong?\n",
    "    - Various schemes for preventing this kind of thing: variance-based split/merge, initialization heuristics\n",
    "## K-Nearest Neighbors (KNN)\n",
    "* Learning Algorithm\n",
    "    - Store training examples\n",
    "* Prediction Algorithm:\n",
    "    - To classify a new example x by finding the training example ($x^i,y^i$) that is *nearest* to x\n",
    "    - Guess the class $y=y^i$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
