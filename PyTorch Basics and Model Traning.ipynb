{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "\n",
    "* Neural Net Traning Workflow\n",
    "\n",
    "* PyTorch Data Type: Tensors\n",
    "\n",
    "* Graph Computation and Neural Net Models\n",
    "\n",
    "* Example: Iris Dataset Classification\n",
    "\n",
    "* Assignment: MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Neural Net Training Workflow\n",
    "1. Prepare Data\n",
    "    * Define batch size\n",
    "    * Split train/val/test sets\n",
    "    * Migrate to Tensors\n",
    "    * Additional pre-processing (normalization, one hot encoding, etc.)\n",
    "\n",
    "    One hot encoding: Transforms categorical data into one hot vectors. 1 in the index representing the class, 0 in all other indices. \n",
    "    \n",
    "    Use *torch.nn.functional.one_hot()* \n",
    "\n",
    "2. Select Hyperparameter\n",
    "    \n",
    "    * Network size and type\n",
    "    * Learning Rate\n",
    "    * Regularizers and strength\n",
    "    * Loss function and optimizer\n",
    "    * Other hyperparameters\n",
    "\n",
    "3. Define Model \n",
    "    * Network type\n",
    "    * Network parameters/layers\n",
    "    * Output values(s) and dimensions\n",
    "    * Forward() Function\n",
    "\n",
    "4. Identiify Tracked Values\n",
    "\n",
    "    * Traning Loss\n",
    "    * Validation Loss\n",
    "    * Other relevant values\n",
    "\n",
    "5. Train and Validate Model\n",
    "\n",
    "    * Train Model: \n",
    "        * Calculate loss on training set\n",
    "        * Backpropagation gradients\n",
    "        * Update weights\n",
    "    * Validate Model:\n",
    "        * Calculate error on validation or test set\n",
    "        * Do not update weights\n",
    "    * Save losses in placeholders\n",
    "\n",
    "6. Visulization and Evaluation\n",
    "    * Visualize Traning Progress: Convergence, over or under fitting\n",
    "    * Evaluate model: Confusion matrix, generate samples, identify model weakness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Tensors\n",
    "\n",
    "* Main data structure for PyTorch\n",
    "* Like numpy arrays, but optimized for machine learning \n",
    "    * Can be migrated to or stored on GPUs\n",
    "    * Optimized for automatic differentiation\n",
    "* Three main attributes:\n",
    "    * Shape - size of each dimension\n",
    "    * Datatype - form of each entry (float, int, etc.)\n",
    "    * Device - cpu or cuda (gpu)\n",
    "\n",
    "**Tensor Initialization**\n",
    "\n",
    "Can create tensor from existing data: *torch.Tensor([[1,2],[3,4]])*, *torch.Tensor(np_array)*\n",
    "\n",
    "Can generate tensor with random or fixed values: *torch.ones(shape)*, *torch.rand(shape)*\n",
    "\n",
    "**Tensor Operations**\n",
    "\n",
    "<img src=\"images/2_3.png\" width =\"500\" height=\"300\" alt=\"centered image\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Graph Computation and Neural Net Models\n",
    "\n",
    "**Bass Class**: nn.Module\n",
    "\n",
    "**Two primary features of base class**: Parameters, Forward function\n",
    "\n",
    "**Common PyTorch Layers**:\n",
    "* Linear\n",
    "* Activation Functions (ReLU, tanj, etc.)\n",
    "* Dropout\n",
    "* RNN\n",
    "* Convolution\n",
    "\n",
    "**Neural Network Models**\n",
    "\n",
    "<img src=\"images/2_4.png\" width =\"700\" height=\"400\" alt=\"centered image\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Computational graphs\n",
    "* PyTorch generates a computational graph every time a parameter or variable with requires_grad is operated on\n",
    "* The graph is used to back-propagate errors and update the parameters\n",
    "\n",
    "### Loss functions and optimizers\n",
    "* **Loss function**: example nn.MSELoss() for regression, nn.NLLLoss() or nn.CrossEntropy() for classification\n",
    "* **Optimizer**: example optim.SGD, optim.Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Optimization during training\n",
    "\n",
    "* Each traning step, the optimization occurs in 3 steps\n",
    "    * optimizer.zero_grad() -Resets the accumulated gradients\n",
    "    * loss.backward() -Back-propagates the gradients to assign the contribution from each parameter\n",
    "    * optimizer.step() -Updates the parameters based on the gradients, according to the optimization scheme (optimizer)\n",
    "**Saving and Loading Models**\n",
    "* Userful quantities to track during traning: training loss, validation loss, model state dictionary (parameters), optimizer state dictionary\n",
    "* Loading state dictionaries: model.load_statedict() loads the saved parameter weights into the model, optimizer.load_statedict() loads optimizer state, such as learning rate, momentum, etc.\n",
    "\n",
    "* Can use torch.save() and torch.load() \n",
    "* Create checkpoints to save all in one file\n",
    "* Can save every eopch, or define some condition for saving (best loss, every n eopchs, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris Dataset Classification\n",
    "\n",
    "\n",
    "```Python\n",
    "from skelarn.datasets import load_iris\n",
    "from sklearn.model_selection import tranin_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataset containing characteristics of 3 different flower types. Found in sklearn.datasets. \n",
    "\n",
    "* 3-layer fully connected neural net\n",
    "* 1^{st} and 2^{nd} layer have 50 neurons each\n",
    "* Final layer has 3 neurons for classification\n",
    "\n",
    "* Activation functions can also be called using torch.nn.functional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition**\n",
    "\n",
    "```Python\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def _init_(self,input_dim):\n",
    "\n",
    "        super(Model,self)._init_()\n",
    "        self.layer1 = nn.Linear(input_dim,50)\n",
    "        self.layer2 = nn.Linear(50,50)\n",
    "        self.layer3 = nn.Linear(50,3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data, hyperparameters, and Saved Values**\n",
    "* Define loss and optimizer\n",
    "\n",
    "* Define training eopchs and data\n",
    "* Tracking loss and accuracy at each epoch\n",
    "\n",
    "```Python\n",
    "import tqdm\n",
    "\n",
    "EPOCHS = 100\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "loss_list = np.zeros((EPOCHS,))\n",
    "accuracy_list = np.zeros(EPOCHS,))\n",
    "\n",
    "model = MOdel(X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Traning and validation**\n",
    "\n",
    "* Output of model gives predictions\n",
    "\n",
    "* Track traning loss as loss\n",
    "\n",
    "* Check accuracy on validation set\n",
    "\n",
    "```Python\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss_list[epoch] = loss.item()\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        correct = (torch.argmax(y_pred, dim=1) == y_test.type(torch.FloatTensor)\n",
    "        accuracy_list[epoch] = correct.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Validation accuracy and training loss**\n",
    "\n",
    "```Python\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(accuracy_list)\n",
    "ax1.set_ylabel('validation accuracy')\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel('training loss')\n",
    "ax2.set_xlabel('eopchs')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
