{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models and Adversaries\n",
    "* Generative Models Definition\n",
    "* Generative Models Taxonomy\n",
    "* Fully Visible BN\n",
    "* Variational Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "From training date to generated samples. Densiity estimation - core problem in unsupervised learning.\n",
    "\n",
    "Example: whichfaceisreal.com\n",
    "\n",
    "**Maximum Likelihood**:\n",
    "$\\theta^* = argmax_{\\theta} E_{x\\sim p_{data}} log p_{model}(x|\\theta) $\n",
    "\n",
    "Explicit - explicitly define and generate $P_{model}$\n",
    "\n",
    "Implicit - generate $P_{model}$ without defining $P_{model}$ exactly\n",
    "\n",
    "## Taxonomy of Generative Models\n",
    "\n",
    "Ian Goodfellow, the inventor. \n",
    "\n",
    "Explicit Density: \n",
    "*   Tractable Density: Fully Visible BN Pixel RNN/CNN, Nonlinear ICA\n",
    "*   Approximate Density: Variational, Markov Chain<br>\n",
    "\n",
    "Implicit Density:\n",
    "*   Direct: GAN\n",
    "*   Markov chain\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FVBNs (Fully visible BN)\n",
    "\n",
    "*   Explicitly formula based on chain rule:<br>\n",
    "$p_{model} (x) = p_{model} (x_1) \\prod_{i=1}^np_{model}(x_i|x_1,x_2,...,x_{i-1})  $<br>\n",
    "\n",
    "* O(n) generation cost\n",
    "* No control through hidden variables\n",
    "\n",
    "**Languale Model**: probability distribution over sequences of words. Given a sequence, say of length m, it assigns a probability to the whole sequence.\n",
    "\n",
    "$$P(W=\\text{SpaceX will *take* me to Mars}) =p1$$\n",
    "$$P(W=\\text{SpaceX will *bake* me to Mars}) =p2$$\n",
    "\n",
    "p2 << p1<br>\n",
    "\n",
    "Chain rule is used to estimate: \n",
    "\n",
    "$P(W) = P(\\text{SpaceX}) P(\\text{will}| \\text{SpaceX}) ..... P(\\text{Mars}| \\text{SpaceX will take me to})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel RNN\n",
    "\n",
    "* Pick a pixel and sample it; Typically top left corner\n",
    "* Connect to neighbor pixels with RNN\n",
    "* Generate neighbors\n",
    "* Continue\n",
    "\n",
    "## WaveNet\n",
    "\n",
    "* A generateive Model for Raw Audio. Oord et al. (2016)\n",
    "* Inpout -> hidden layers (Conv) -> a single time step output\n",
    "\n",
    "## Variational Autoencoder\n",
    "\n",
    "Idea is, instead of using the chain rule of products, substitute the sequential variables with variables z (latent/hidden variable) to generate the output.\n",
    "\n",
    "$\\displaystyle p_{model} (x) = p_{model} (x_1) \\prod_{i=2}^np_{model}(x_i|x_1,x_2,...,x_{i-1})  $<br> -->\n",
    "$\\displaystyle p_{model} (x) = \\int p_{model}(z) p_{model}(x|z)dz  $<br>\n",
    "\n",
    "\n",
    "**Structurally**: \n",
    "Input x -> x  + encoder -> hidden z -> z+ decoder -> generative output (loss)\n",
    "\n",
    "* Choose density distribution: eg, Gaussian\n",
    "* Define a network to generate the conditional\n",
    "\n",
    "$p_{model}(z|x) = p_{model}(x|z)p_{model}(z)/p_{model}(x) $\n",
    "\n",
    "$p_{model}(x) $ is intractable, need to approximate. The solution is after encoder mapping input x to z, z -> gaussian mean, std -> sample x|z from Gaussian distribution\n",
    "\n",
    "**Putting it all together**\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_1.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning with NN\n",
    "\n",
    "**Data**:\n",
    "{x} x: inputs No labels\n",
    "* Can have much more data\n",
    "* Challenge: Cost?\n",
    "\n",
    "Neural Network Goal:\n",
    "Learn a structure if the data\n",
    "* Has the potential to learn real world\n",
    "* Challenge: Optimization?\n",
    "\n",
    "### Kullback-Leibler (KL) Divergence\n",
    "\n",
    "$D_{KL}(P||Q) = \\sum_x P(x) log(\\frac{P(x)}{Q(x)}) $\n",
    "\n",
    "integral: $D_{KL}(P||Q) = \\int_{-\\infty}^{\\infty} P(x) log(\\frac{P(x)}{Q(x)})\\,dx $\n",
    "\n",
    "### Log Likelihood Expression\n",
    "\n",
    "Loss($\\theta, x_i$)$ =-E_{z\\sim q^{\\phi}_{model}}(z|x^{(i)})[log p^{\\theta}_{model}(x^{(i)}|z)]+ D_{KL}[q^{\\phi}_{model}(z|x^{(i)})||p^{\\phi}_{model}(z)] +D_{KL}[q^{\\phi}_{model}(z|x^{(i)})||p^{\\phi}_{model}(z|x^{(i)})] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs\n",
    "* Use Latent Variables\n",
    "* Asymptotically Consistent (unlike VAG)\n",
    "* Potentially Can Reach Global Optimum\n",
    "* No Markov Chains Needed\n",
    "\n",
    "* Unprincipled\n",
    "* Could Take Long Time to Converge\n",
    "\n",
    "**GANs** - Two player game\n",
    "* Instead of sampling from high-dimensional, complex and unknown distribution\n",
    "* Sample from simple distribution, e.g. normal distribution (random noise) and find the transformation to the distribution we want to learn\n",
    "* Learn the transformation using NNs\n",
    "\n",
    "* Generator - try to generate samples and present them as real world and fool the discriminator\n",
    "* Discriminator - try to distinguish between real and generated (fake) images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network \n",
    "\n",
    "z (simple distribution) -> Neural Network G (CNNs, RNNs, or other) -> Ouput Sample $x^G$\n",
    "\n",
    "Traning data has distribution $p_{data}$. Sample $x\\sim p_{data}.$  Require: Output sample $x^G$ is of similar dimensions as $x$ and distribution $p_{data}$\n",
    "\n",
    "### Discriminator \n",
    "\n",
    "Input sample x -> Neural Network D  -> 0/1 \n",
    "\n",
    "Receives input of same dimension as $p_{data}$\n",
    "Determine: is sample from $p_{data}$ (1) or not (0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing Cost Functions\n",
    "\n",
    "$$J^{(D)}= -\\frac{1}{2}\\mathbb{E}_{x\\sim p_{data}}log D_{\\theta_d}(x)-\\frac{1}{2}\\mathbb{E}_{z\\sim p_{model}}log(1-D_{\\theta_d}(G_{\\theta_g}(z))) \\\\ J^{(G)}=-J^{(D)}\\\\\\\\\n",
    "or, \\, J^{(D)}= -\\frac{1}{2}\\int p_{data}(x)log D(x)\\,dx-\\frac{1}{2}\\int p_{model}(x)log(1-D(x))\\,dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: \n",
    "* Instead of $\\theta_d$ assume that we optimize D(x) for every value of x. What would be the optimal strategy for D(x)?\n",
    "* What assumptions are needed?\n",
    "\n",
    "$$\\frac{\\partial}{\\partial_D}J^{(D)} = -\\frac{1}{2}\\int (\\frac{p_{data}}{D} + \\frac{p_{model}}{D-1})=0\n",
    "\\\\\n",
    "\\rightarrow\\; D(x) = \\frac{p_{data}}{p_{model}+p_{data}} $$\n",
    "\n",
    "Assumption: $p_{model},\\, p_{data}$ are nonzero everywhere\n",
    "\n",
    "Equilibrium: $p_{model}=p_{data}$ then, $D(x)=\\frac{1}{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax Game Optimization\n",
    "\n",
    "$$min_{\\theta_g}max_{\\theta_d}[E_{x\\sim p_{data}}logD_{\\theta_d}(x)+E_{z\\sim p_{model}}log(1-D_{\\theta_d}(G_{\\theta_g}(z)))] $$\n",
    "\n",
    "Solution:\n",
    "* Saddle point in the parameter space (Nash Equilibrium)\n",
    "* Similar to solution of Jensen-Shannon Divergence\n",
    "\n",
    "\n",
    "* **Gradient ascent** for the discriminator on J, **Gradient descent** for the generator\n",
    "$$J^{(D)}=\\frac{1}{2}\\mathbb{E}_{x\\sim p_{data}}logD_{\\theta_d}(x)+\\frac{1}{2}\\mathbb{E}_{z\\sim p_{model}}log(1-D_{\\theta_d}(G_{\\theta_g}(z)))\\\\$$\n",
    "$$J^{(G)}=-\\frac{1}{2}\\mathbb{E}_{z\\sim p_{model}}log(1-D_{\\theta_d}(G_{\\theta_g}(z))) $$\n",
    "\n",
    "### Saturation \n",
    "\n",
    "Gradient is slow near 0. Solution is to both gradient ascent:\n",
    "$$J^{(D)}=\\frac{1}{2}\\mathbb{E}_{x\\sim p_{data}}logD_{\\theta_d}(x)+\\frac{1}{2}\\mathbb{E}_{z\\sim p_{model}}log(1-D_{\\theta_d}(G_{\\theta_g}(z)))\\\\$$\n",
    "$$J^{(G)}=\\frac{1}{2}\\mathbb{E}_{z\\sim p_{model}}log(D_{\\theta_d}(G_{\\theta_g}(z))) $$\n",
    "\n",
    "### DCGAN\n",
    "\n",
    "Fully connected -> convolutional layer -> generated image\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_2.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### Similarity in Hidden Space\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_3.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### Text -> Image Synthesis\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_4.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### Cycle GAN\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_5.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### Pix2Pix\n",
    "\n",
    "<div class=\"verticalhorizontal\">\n",
    "    <img src=\"images/9_6.png\" width =\"450\" height=\"300\" alt=\"centered image\" />\n",
    "</div>\n",
    "\n",
    "### The GAN Zoo\n",
    "\n",
    "github examples of GANs: github.com/hindupuravinash/the-gan-zoo\n",
    "\n",
    "### Relation with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo Algorithm:\n",
    "\n",
    "1. Take simple distribution $z$ as input, forward pass to get $G(z)$ or $x^G$. \n",
    "\n",
    "2. Shuffle the $x^G$ and real images $x$. Record the index and labels for computing loss in the future.\n",
    "\n",
    "3. Combine the shuffled data and forward pass to get $D(G(z))$. Compute the loss. \n",
    "\n",
    "4. For backward pass, compute the loss gradient $J^{(D)}$ w.r.t $\\theta_d$ first. Then compute the loss gradient $J^{(G)}$ w.r.t $\\theta_g$.\n",
    "\n",
    "5. Update $\\theta_d$, $\\theta_g$  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
